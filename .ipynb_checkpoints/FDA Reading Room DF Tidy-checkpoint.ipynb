{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDA Reading Room Dataframe Tidying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import requests\n",
    "import lxml\n",
    "import html5lib \n",
    "from bs4 import BeautifulSoup\n",
    "import janitor\n",
    "import tempfile\n",
    "import os # to get current opperating system\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\ncruickshank\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# selenium functions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys #allow  you to enter keystrokes into fields\n",
    "from selenium.webdriver.support.ui import Select #allow you to select a dropdown item\n",
    "from selenium.webdriver.support.ui import WebDriverWait #lets you modify a field before proceeding\n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_room = pd.read_csv(\"data/FDA Reading Room Form 483 Dataset - Last Updated 20210512.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim one set of observations to figure out the syntax\n",
    "Test case is Edge Pharma Form 483 from 03/30/2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process test and separate into different OBSERVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate test case\n",
    "test = reading_room.observations[11] # 11 = edge pharma\n",
    "\n",
    "# cut the preamble before OBSERVATION 1 and strip leading and lagging whitespace\n",
    "preamble = 'This document lists observations made by the FDA representative(s) during the inspection of your facility. They are inspectional observations, and do not represent a final Agency determination regarding your compliance. If you have an objection regarding an observation, or have implemented, or plan to implement, corrective action in response to an observation, you may discuss the objection or action with the FDA representative(s) during the inspection or submit this information to FDA at the address above. If you have any questions, please contact FDA at the phone number and address above.  DURING AN INSPECTION OF YOUR FIRM WE OBSERVED: '\n",
    "str_start = test.find(preamble) + len(preamble)\n",
    "test = test[str_start:].strip()\n",
    "\n",
    "# replace carraige returns with regular whitespace\n",
    "test = re.sub(r\"(\\n|\\x0c)\", \" \", test).strip()\n",
    "\n",
    "# split the text into different strings based on OBSERVATIONS\n",
    "observations = re.split(r\"OBSERVATION [0-9]+\\s+\", test)\n",
    "cleaned_observations = []\n",
    "for observation in observations:\n",
    "        \n",
    "    # split observation into words\n",
    "    tokens = nltk.word_tokenize(observation)\n",
    "    \n",
    "    # removeall non-alphabetic words and convert to lower case\n",
    "    words# = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # filter out stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if not word in stop_words]\n",
    "    \n",
    "    # stem the verbs\n",
    "    #porter = nltk.PorterStemmer()\n",
    "    #stemmed = [porter.stem(word) for word in words]\n",
    "    \n",
    "    def list_to_string(s):\n",
    "        str1 = \" \"\n",
    "        return (str1.join(s))\n",
    "    \n",
    "    clean_observation = list_to_string(words)\n",
    "    \n",
    "    cleaned_observations.append(clean_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_observations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
